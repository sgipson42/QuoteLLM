**Overview:**

This research project is currently ongoing as an independent study with a student partner and two faculty advisors. It began in Fall 2023 in conjunction with the Computing Research Association UR2PhD online program, aimed to engage undergraduate women interested in pursuing a graduate degree in computer science. The UR2PhD course honed my ability to think about a research topic in the context of the modern and advancing computer science field in order to develop insightful methods and findings. I also learned foundational research skills and methods through this course, which I have honed and practiced throughout writing and presenting a research proposal on and doing the following research. 

**Research Topic:**

Investigate the extent to which OpenAI's Large Language Models can accurately quote publicly available online texts in the context of copyrighted training data. This topic is highly relevant to the state of AI today and its future development, as many authors as well as the New York Times have filed lawsuits against OpenAI for copyright infringement, wherein ChatGPT is able to summarize entrie books, or in the case of the New York Times, compete as a news outlet. The basis for these lawsuits comes from OpenAI's models having been trained on data found online, including much copyrighted data. My team's goal is to answer the question of whether or not these LLMs are able to reproduce text from a variety of corpora by testing the ability of both GPT-3.5 and GPT-4 to complete random quotes from various corpora found online. We aim to answer a variety of related questions throughout the course of this research, with a few listed below.

**Research Questions:**

1. To what extent are GPT-3.5 and GPT-4 able to reproduce text?
2. How can we quantitatively measure the accuracy of GPT's responses?
3. How must GPT be prompted to complete a quote most accurately? How do responses differ between the two models?
4. How do the quotability copyrighted and uncopyrighted works compare? Similarly, are works found more frequently online more quotable than rare works?
5. What role does syntax play in GPT's ability to quote?

**Tools Used:**

1. OpenAI API
2. SpaCy tokenization library
3. OpenAI tokenizer (Tiktoken)
5. Python libraries (e.g. Pandas, NumPy, Matplotlib, Seaborn, Sentence Transformers)

**Improved Technical Skills:**
1. Natural Language Processing: methodologies, implementation techniques and use cases (tokenization, embeddings, prompting)
2. Programmatic data analysis and visualization
3. Python Scripting

